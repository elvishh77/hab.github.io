<!DOCTYPE html>
<html>
<head>
  <!-- Keep your LISAt-style stack -->
  <script src="static/css/carousel.js"></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Social cards -->
  <meta name="description" content="IVA: Instruct, Verify, Act — Teaching VLA Models to Reject the Impossible. IVA detects false-premise instructions, clarifies or corrects them in language, and then acts safely.">
  <meta property="og:title" content="IVA: Instruct, Verify, Act" />
  <meta property="og:description" content="Teaching VLA Models to Reject the Impossible — Detect the impossible. Clarify intent. Act safely." />
  <meta property="og:url" content="http://iva-vla.github.io" />
  <meta property="og:image" content="static/images/fig1_teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="IVA: Instruct, Verify, Act">
  <meta name="twitter:description" content="Teaching VLA Models to Reject the Impossible — an unified framework to detect false-premise instructions, clarify them, and act safely.">
  <meta name="twitter:image" content="static/images/fig1_teaser.png">
  <meta name="twitter:card" content="summary_large_image">

  <!-- Keywords -->
  <meta name="keywords" content="Vision-Language-Action, Robotics, False Premise, Instruction Following, Human-Robot Interaction">

  <title>IVA: Instruct, Verify, Act</title>

  <!-- Fonts & CSS -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/fontawesome/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- JS -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/fontawesome/js/fontawesome.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!-- ===== Responsive helpers (kept minimal) ===== -->
  <style>
    /* Title row defaults (desktop) */
    .publication-title-nowrap { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
    .title-row { display: inline-flex; align-items: center; gap: 12px; }
    .title-row img { width: 128px; height: 128px; border-radius: 12px; }

    /* Figure helpers */
    .fig-medium { max-width: 900px; margin: 1rem auto; }
    .fig1 img { width: 150%; margin: auto; display: block; }
    .fig-wide img { width: 150%; margin: auto; display: block; } /* Table 1 & Fig C.2 for consistency */

    /* Tablets */
    @media (max-width: 768px) {
      .publication-title-nowrap { white-space: normal; }
      .title-row img { width: 96px; height: 96px; }
      .fig1 img { width: 80%; }
      .fig-wide img { width: 85%; }
    }

    /* Phones */
    @media (max-width: 480px) {
      .title-row { flex-direction: column; align-items: center; }
      .publication-title-nowrap {
        white-space: normal;
        font-size: 1.75rem !important;  /* shrink instead of clipping */
        line-height: 1.15;
        text-align: center;
      }
      .subtitle.is-4 { font-size: 1.1rem; line-height: 1.3; }
      .title-row img { width: 84px; height: 84px; }
      .fig1 img { width: 90%; }
      .fig-wide img { width: 92%; }
    }

    /* Very small phones */
    @media (max-width: 360px) {
      .publication-title-nowrap { font-size: 1.55rem !important; }
      .title-row img { width: 72px; height: 72px; }
    }
  </style>
</head>

<body>

  <!-- ===================== HERO ===================== -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered is-vcentered">
          <div class="column has-text-centered is-four-fifths is-vcentered">
            <!-- Title + logo (stacks on mobile) -->
            <div class="title-row">
              <img src="static/images/iva_icon.png" alt="IVA icon">
              <h1 class="title is-1 publication-title publication-title-nowrap" style="margin:0;">
                IVA: Instruct, Verify, Act
              </h1>
            </div>
            <p class="subtitle is-4" style="margin-top:.5rem;">
              Teaching VLA Models to Reject the Impossible
            </p>
          </div>
        </div>

        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered is-four-fifths">
              <!-- Authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://wen-hanhsieh.github.io/" target="_blank">Wen-Han Hsieh *</a>,</span>
                <span class="author-block"><a href="https://elvishh77.github.io/" target="_blank">Elvis Hsieh *</a>,</span>
                <span class="author-block"><a href="https://dantong88.github.io/" target="_blank">Dantong Niu</a>,</span>
                <span class="author-block"><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,</span>
                <span class="author-block"><a href="https://roeiherz.github.io/" target="_blank">Roei Herzig</a>,</span>
                <span class="author-block"><a href="https://dchan.cc/" target="_blank">David M. Chan</a></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">UC Berkeley</span>
              </div>

              <div class="is-size-6 publication-authors has-text-centered">
                <span class="author-block">EMNLP 2025 (Findings)</span>
              </div>

              <br>
              <!-- Links -->
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2508.16292" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="ai ai-arxiv"></i></span><span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/Wen-HanHsieh" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fa-brands fa-github"></i></span><span>Code &amp; Data: Coming Soon</span>
                    </a>
                  </span>
                </div>
              </div>

            </div>
          </div>
        </div>

      </div>
    </div>
  </section>

  <!-- ===================== ABSTRACT ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Abstract</h2>
          <div class="content has-text-justified">
            <!-- Paper-faithful wording -->
            <p>
              IVA is <em>an unified framework</em> for Vision-Language-Action (VLA) models that
              <em>detects</em> when an instruction is unfulfillable (false premise), <em>clarifies or corrects</em> it in natural language,
              and then <em>acts safely</em>. Trained with paired true- and false-premise instructions,
              IVA improves false-premise handling while maintaining strong true-premise task performance.
            </p>
          </div>

          <!-- Figure 1 (after abstract, sized responsively) -->
          <figure class="image fig-medium fig1">
            <img src="static/images/iva_gif_new.gif"
                 alt="Figure 1: IVA detects a false-premise instruction, clarifies, and proposes a valid alternative." />
            <figcaption>Figure 1: IVA detects a false-premise instruction, clarifies, and proposes a valid alternative.</figcaption>
          </figure>

        </div>
      </div>
    </div>
  </section>

  <!-- ===================== WHY IVA ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Why IVA?</h2>
          <div class="content has-text-justified">
            <ul>
              <li>Most VLAs assume user instructions are always feasible; in practice, people often issue commands with <b>false premises</b> (missing objects, attribute mismatches, or impossible actions).</li>
              <li>Naïvely following such instructions is unsafe or wasteful. IVA introduces explicit <b>detection</b> and <b>clarification/correction</b> to keep interaction safe and productive.</li>
              <li>We evaluate both <b>In-Domain</b> (plausible-but-absent) and <b>Out-of-Domain</b> (impossible/absurd) false premises, and train IVA to handle both categories.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== METHOD ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Our Solution</h2>
          <div class="content has-text-justified">
            <ul>
              <li><b>Single-stage end-to-end tuning:</b> IVA jointly learns <i>false-premise detection</i>, <i>language clarification/correction</i>, and <i>action prediction</i> (unlike two-stage baselines).</li>
              <li><b>Inputs &amp; Outputs:</b> front camera and the previous <b>5 joint positions (angles)</b> → predict the 2-D visual trace and the next action as an <b>8-D vector</b> (<b>7 joint velocities</b> + binary gripper).</li>
              <li><b>Architecture:</b> frozen vision &amp; language encoders; an autoregressive decoder fine-tuned with <i>LoRA</i> adapters for efficient training.</li>
              <li><b>Instruction template:</b> structured prompt with robot type, control mode, task, and a short proprioceptive history enables grounded reasoning and safe actions.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== DATASET ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">How did we construct the dataset?</h2>
          <div class="content has-text-justified">
            <ul>
              <li>Built on <b>RLBench</b> trajectories across <b>9 tasks</b>, with paired <i>true</i> and <i>false-premise</i> prompts.</li>
              <li><b>800 episodes per task</b>; false premises are injected at <b>10%</b> of the steps: about <b>65%</b> In-Domain FP episodes and <b>20%</b> Out-of-Domain FP episodes (rest true-premise).</li>
              <li>In-Domain FP: plausible but absent objects (e.g., “open the middle block” in a drawer scene). Out-of-Domain FP: clearly infeasible requests (e.g., “open the top elephant”).</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== EVALUATION ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Evaluation</h2>
          <div class="content has-text-justified">
            <ul>
              <li><b>Episodes:</b> 9 RLBench tasks × 25 episodes each = <b>225 episodes</b> with randomized object poses and paired prompts.</li>
              <li><b>(1) Detection:</b> parse the model’s text to classify <i>Accept</i> (true-premise) vs <i>Clarify/Refuse</i> (false-premise); average per-step FP scores per episode.</li>
              <li><b>(2) Execution:</b> when IVA accepts, execute the predicted <b>8D joint-velocity</b> sequence; task success judged by the RLBench success detector.</li>
              <li><b>(3) Overall:</b> average Detection + Execution over all 225 episodes into a single accuracy metric.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== RESULTS ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Results</h2>

          <div class="content has-text-justified">
            <ul>
              <li><b>False-premise detection:</b> <b>100%</b> (In-Domain) and <b>97.78%</b> (Out-of-Domain).</li>
              <li><b>Improvement over baseline:</b> IVA markedly boosts FP detection and increases successful responses under FP scenarios (see Table&nbsp;1 per-task details).</li>
              <li><b>True-premise performance:</b> IVA maintains strong success (<b>42.67% ± 8.34%</b>) comparable to LLaRVA (<b>38.67% ± 8.55%</b>).</li>
            </ul>
          </div>

          <!-- Table 1 -->
          <figure class="image fig-medium fig-wide">
            <img src="static/images/table1_results.png" alt="Table 1: IVA vs. LLaRVA across 9 RLBench tasks">
            <figcaption>Table 1: Per-task Overall, False-Premise Detection (ID/OOD), and True-Premise Success across 9 RLBench tasks.</figcaption>
          </figure>

          <!-- Figure 2 -->
          <figure class="image fig-medium fig-wide">
            <img src="static/images/figC2_tasks.png" alt="Figure 2: Qualitative examples across 9 RLBench tasks">
            <figcaption>Figure 2: Qualitative examples across 9 RLBench tasks showing IVA's false-premise handling and safe alternative suggestions.</figcaption>
          </figure>

        </div>
      </div>
    </div>
  </section>

  <!-- ===================== TAKEAWAYS ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Key Takeaways</h2>
          <div class="content has-text-justified">
            <ul>
              <li>IVA explicitly reasons about <b>feasibility</b>, clarifies impossible requests, and proposes valid alternatives—leading to safer HRI.</li>
              <li>Robust FP handling <b>does not</b> degrade feasible task performance.</li>
              <li>The framework is general and extendable to broader robotic tasks, sensors, and interaction settings.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== BIBTEX ===================== -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{hsieh2025do,
    title     = {Do What? Teaching Vision-Language-Action Models to Reject the Impossible},
    author    = {Wen-Han Hsieh and Elvis Hsieh and Dantong Niu and Trevor Darrell and Roei Herzig and David M. Chan},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2025},
    year      = {2025},
    url       = {https://arxiv.org/abs/2508.16292}
}</code></pre>
    </div>
  </section>

  <!-- ===================== FOOTER ===================== -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>
              which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the structure of this website; please link back in the footer.
              <br>This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
