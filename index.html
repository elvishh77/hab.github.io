<!DOCTYPE html>
<html>
<head>
  <script src="static/css/carousel.js"></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Social cards -->
  <meta name="description" content="Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation">
  <meta property="og:title" content="Seg the HAB" />
  <meta property="og:description" content="Language-Guided Geospatial Algae Bloom Reasoning and Segmentation" />
  <meta property="og:url" content="http://hab.github.io" />
  <meta property="og:image" content="static/images/hab_teaser.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="Seg the HAB">
  <meta name="twitter:description" content="Language-Guided Geospatial Algae Bloom Reasoning and Segmentation">
  <meta name="twitter:image" content="static/images/hab_teaser.png">
  <meta name="twitter:card" content="summary_large_image">

  <!-- Keywords -->
  <meta name="keywords" content="Harmful Algal Blooms, Remote Sensing, Vision-Language Models, Segmentation, Climate Change">

  <title>Seg the HAB</title>

  <!-- Fonts & CSS -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/fontawesome/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- JS -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/fontawesome/js/fontawesome.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!-- Responsive helpers -->
  <style>
    .publication-title-nowrap { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; }
    .title-row { display: inline-flex; align-items: center; gap: 12px; }
    .title-row img { width: 128px; height: 128px; border-radius: 12px; }

    .fig-medium { max-width: 900px; margin: 1rem auto; }
    .fig1 img { width: 100%; margin: auto; display: block; }
    .fig-wide img { width: 100%; margin: auto; display: block; }

    @media (max-width: 768px) {
      .publication-title-nowrap { white-space: normal; }
      .title-row img { width: 96px; height: 96px; }
      .fig1 img { width: 100%; }
      .fig-wide img { width: 100%; }
    }

    @media (max-width: 480px) {
      .title-row { flex-direction: column; align-items: center; }
      .publication-title-nowrap {
        white-space: normal;
        font-size: 1.75rem !important;
        line-height: 1.15;
        text-align: center;
      }
      .subtitle.is-4 { font-size: 1.1rem; line-height: 1.3; }
      .title-row img { width: 84px; height: 84px; }
      .fig1 img { width: 100%; }
      .fig-wide img { width: 100%; }
    }

    @media (max-width: 360px) {
      .publication-title-nowrap { font-size: 1.55rem !important; }
      .title-row img { width: 72px; height: 72px; }
    }
  </style>
</head>

<body>

  <!-- ===================== HERO ===================== -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered is-vcentered">
            <div class="column has-text-centered is-four-fifths is-vcentered">
              <!-- Title + logo (stacks on mobile) -->
              <div class="title-row" style="display: flex; align-items: center; justify-content: center; flex-wrap: wrap;">
                <img src="static/images/algos_icon.png" alt="ALGOS icon" style="width: 100px; height: auto;">
                <h1 class="title is-1 publication-title publication-title-nowrap" style="margin:0;">
                  Seg the HAB
                </h1>
              </div>
            </div>
        </div>

        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered is-four-fifths">
              <!-- Authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block"><a href="https://www.linkedin.com/in/pattersonhsieh/" target="_blank">Patterson Hsieh<sup>1,*</sup></a>,</span>
                <span class="author-block"><a href="https://www.linkedin.com/in/jryeh/" target="_blank">Jerry Yeh<sup>2,*</sup></a>,</span>
                <span class="author-block"><a href="https://www.linkedin.com/in/mao-chi-he/" target="_blank">Mao-Chi He<sup>2,*</sup></a>,</span>
                <span class="author-block"><a href="https://wen-hanhsieh.github.io/" target="_blank">Wen-Han Hsieh<sup>2</sup></a>,</span>
                <span class="author-block"><a href="https://elvishh77.github.io" target="_blank">Elvis Hsieh<sup>2</sup></a></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>UC San Diego, <sup>2</sup>UC Berkeley</span>
              </div>

              <div class="is-size-6 publication-authors has-text-centered">
                <span class="author-block">Tackling Climate Change with Machine Learning @ NeurIPS 2025</span>
              </div>

              <div class="is-size-6 publication-authors has-text-centered" style="margin-top: 0.5rem;">
                <span class="author-block"><sup>*</sup>Equal Contribution</span>
              </div>

              <br>
              <!-- Links -->
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/submit/6911290" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="ai ai-arxiv"></i></span><span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/algos-hab" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fa-brands fa-github"></i></span><span>Code &amp; Data: Coming Soon</span>
                    </a>
                  </span>
                </div>
              </div>

            </div>
          </div>
        </div>

      </div>
    </div>
  </section>

  <!-- ===================== ABSTRACT ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Climate change is intensifying the occurrence of harmful algal blooms (HAB), particularly cyanobacteria, 
              which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption 
              of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive 
              and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote 
              sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery 
              and quantifying bloom severity.
            </p>
            <p>
              In this work, we introduce <strong>ALGOS (ALGae Observation and Segmentation)</strong>, a segmentation-and-reasoning 
              system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach 
              integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision 
              language models on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. 
              Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, 
              paving the way toward practical and automated cyanobacterial monitoring systems.
            </p>
          </div>

          <!-- Pipeline Figure -->
          <figure class="image fig-medium fig1">
            <img src="static/images/pipeline.png"
                 alt="Figure 1: ALGOS pipeline showing multimodal LLM with SAM decoder" />
            <figcaption><strong>Figure 1:</strong> The pipeline of ALGOS. Given the input image and text query, the multimodal LLM generates text output. The last-layer embedding for the &lt;SEG&gt; token is then decoded into the segmentation mask via the SAM decoder.</figcaption>
          </figure>

        </div>
      </div>
    </div>
  </section>

  <!-- ===================== MOTIVATION ===================== -->
  <section class="section hero" style="background-color: #f5f5f5;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">The Problem</h2>
          <div class="content has-text-justified">
            <ul>
              <li><strong>Ecological & Health Impact:</strong> HAB cause mass fish mortality, ecosystem disruption, and release toxins harmful to humans and wildlife.</li>
              <li><strong>Economic Burden:</strong> The 2017-2018 Florida Red Tide alone caused an estimated $2.7 million in losses, with billions lost annually worldwide.</li>
              <li><strong>Monitoring Limitations:</strong> Existing methods rely on manual water sampling and microscopy—costly, time-consuming, and geographically constrained.</li>
              <li><strong>Fragmented AI Solutions:</strong> Prior work addresses either bloom severity prediction <em>or</em> spatial segmentation in isolation, limiting comprehensive monitoring capabilities.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== METHOD ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Our Solution: ALGOS</h2>
          <div class="content has-text-justified">
            <p>ALGOS is a unified vision-language framework that bridges reasoning segmentation with HAB severity assessment in satellite imagery.</p>
            
            <h3 class="title is-5" style="margin-top: 1.5rem;">Dataset Curation</h3>
            <ul>
              <li><strong>HAB Segmentation Dataset:</strong> GeoSAM-assisted annotation with human evaluation on CAML Sentinel-2 imagery. Interactive mask generation with positive/negative prompts and ROI boxes, followed by morphological filtering and human validation.</li>
              <li><strong>HAB Reasoning Dataset:</strong> Five-level severity scale based on WHO recreational guidance (Level 1: &lt;2×10⁴ to Level 5: ≥1×10⁷ cells/mL). Natural language query templates paired with satellite images and severity labels.</li>
            </ul>

            <h3 class="title is-5" style="margin-top: 1.5rem;">Architecture</h3>
            <ul>
              <li><strong>Vision Encoder:</strong> Remote-CLIP ViT-L/14 optimized for satellite imagery</li>
              <li><strong>Language Model:</strong> Vicuna-7B with specialized &lt;SEG&gt; token for segmentation</li>
              <li><strong>Decoder:</strong> SAM decoder head for pixel-level mask prediction</li>
              <li><strong>Training:</strong> Joint optimization with text generation loss and segmentation loss (BCE + DICE)</li>
            </ul>

            <h3 class="title is-5" style="margin-top: 1.5rem;">Implementation</h3>
            <ul>
              <li>Trained on 8× NVIDIA DGX A100 (80GB) GPUs</li>
              <li>LoRA adapters for efficient fine-tuning</li>
              <li>Joint training on HAB, FP-Ref-COCO, and ReasonSeg datasets</li>
              <li>~6 hours training time</li>
            </ul>
          </div>

          <!-- Interactive Segmentation Workflow -->
          <figure class="image fig-medium fig-wide" style="margin-top: 2rem;">
            <img src="static/images/segmentation_workflow.png" alt="Interactive segmentation workflow" style="max-width: 70%; margin: 20px auto; display: block;">
            <figcaption><strong>Figure 2:</strong> Interactive segmentation workflow used during data curation. (a) User-provided prompts (green: positive, red: negative), (b) Generated mask overlay, (c) Bounding boxes extracted.</figcaption>
          </figure>

        </div>
      </div>
    </div>
  </section>

  <!-- ===================== RESULTS ===================== -->
  <section class="section hero" style="background-color: #f5f5f5;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Results</h2>

          <div class="content has-text-justified">
            <p>ALGOS achieves strong performance across both segmentation and severity prediction tasks:</p>
            
            <h3 class="title is-5" style="margin-top: 1.5rem;">Segmentation Performance</h3>
            <ul>
              <li><strong>cIoU (per-image):</strong> 0.6493 (vs. LISAT: 0.1083, LISA-7B: 0.1373)</li>
              <li><strong>gIoU (dataset-level):</strong> 0.5969 (vs. LISAT: 0.1052, LISA-7B: 0.1274)</li>
              <li>ALGOS far surpasses baseline models in accurately capturing both per-image bloom regions and large contiguous extents</li>
            </ul>

            <h3 class="title is-5" style="margin-top: 1.5rem;">Severity Prediction</h3>
            <ul>
              <li><strong>MSE:</strong> 2.984 (vs. LLaVA-7B: 3.868) — 22.8% reduction</li>
              <li><strong>RMSE:</strong> 1.727 (vs. LLaVA-7B: 1.967)</li>
              <li><strong>MAE:</strong> 1.365 (vs. LLaVA-7B: 1.587)</li>
            </ul>
          </div>

          <!-- Results Tables -->
          <div style="display: flex; gap: 2rem; flex-wrap: wrap; justify-content: center; margin-top: 2rem;">
            <figure class="image" style="max-width: 400px;">
              <img src="static/images/table1_segmentation.png" alt="Table 1: Segmentation Results" style="max-width: 80%; margin: 20px auto; display: block;">
              <figcaption><strong>Table 1:</strong> Segmentation results comparing LISAT, LISA, and ALGOS.</figcaption>
            </figure>
            <figure class="image" style="max-width: 400px;">
              <img src="static/images/table2_severity.png" alt="Table 2: Severity Prediction Results" style="max-width: 80%; margin: 20px auto; display: block;">
              <figcaption><strong>Table 2:</strong> Severity prediction results comparing LLaVA baseline and ALGOS.</figcaption>
            </figure>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- ===================== QUALITATIVE RESULTS ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Qualitative Comparison</h2>
          
          <figure class="image fig-wide" style="margin-top: 2rem;">
            <img src="static/images/qualitative_comparison.png" alt="Qualitative comparison across models" style="max-width: 70%; margin: 20px auto; display: block;">
            <figcaption><strong>Figure 3:</strong> Qualitative comparison of predictions across LISA-7B, LISAT, and ALGOS models. ALGOS demonstrates superior bloom localization and boundary delineation across diverse query types.</figcaption>
          </figure>

        </div>
      </div>
    </div>
  </section>

  <!-- ===================== KEY TAKEAWAYS ===================== -->
  <section class="section hero" style="background-color: #f5f5f5;">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Key Contributions</h2>
          <div class="content has-text-justified">
            <ul>
              <li><strong>Unified Framework:</strong> First system to jointly perform spatial segmentation and severity-level estimation for HAB monitoring in satellite imagery.</li>
              <li><strong>High-Quality Dataset:</strong> Semi-supervised curation pipeline combining GeoSAM automation with human validation for robust segmentation masks.</li>
              <li><strong>Strong Performance:</strong> Significant improvements over baselines in both segmentation (6× cIoU increase) and severity prediction (23% MSE reduction).</li>
              <li><strong>Scalable Monitoring:</strong> Enables automated, wide-area HAB monitoring for ecological assessment and public health decision-making.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== LIMITATIONS & FUTURE WORK ===================== -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-4">Limitations & Future Work</h2>
          <div class="content has-text-justified">
            <ul>
              <li><strong>Geographic Scope:</strong> Current evaluation limited to CAML dataset regions and seasons. Generalization to diverse aquatic environments requires larger-scale, cross-region benchmarks.</li>
              <li><strong>Data Dependencies:</strong> Reliance on curated datasets highlights the need for continuous data integration pipelines that adapt to evolving ecological conditions.</li>
              <li><strong>Future Directions:</strong> Extending evaluations to global waterbodies, incorporating temporal dynamics, and developing real-time monitoring capabilities for operational deployment.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===================== BIBTEX ===================== -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hsieh2025segthehab,
    title     = {Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation},
    author    = {Patterson Hsieh and Jerry Yeh and Mao-Chi He and Wen-Han Hsieh and Elvis Hsieh},
    journal   = {NeurIPS 2025 Workshop on Tackling Climate Change with Machine Learning},
    year      = {2025},
    note      = {arXiv:submit/6911290 [cs.AI]}
}</code></pre>
    </div>
  </section>

  <!-- ===================== FOOTER ===================== -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>
              which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the structure of this website; please link back in the footer.
              <br>This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>